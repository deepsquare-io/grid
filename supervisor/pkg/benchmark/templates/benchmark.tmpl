{{- define "benchmark" -}}
#!/bin/bash

#SBATCH -N {{ .JobDefinition.MinNodes }}-{{ .JobDefinition.MaxNodes }}
#SBATCH --ntasks={{ .JobDefinition.NTasks }}
#SBATCH --ntasks-per-node={{ .JobDefinition.NTasksPerNode }}
#SBATCH --mem=0
#SBATCH --mincpus={{ .JobDefinition.CPUsPerNode }}
#SBATCH --gpus-per-node={{ .JobDefinition.GPUsPerNode }}
#SBATCH --cpus-per-task={{ div .JobDefinition.CPUsPerNode .JobDefinition.NTasksPerNode }}

# Select obi-wan as MPI P2P communications
export PMIX_MCA_pml=ob1
# Select shared-memory as Byte-Transport Layer
export PMIX_MCA_btl=vader,self,tcp
export OMPI_MCA_pml=ob1
export OMPI_MCA_btl=vader,self,tcp

GPU_AFFINITY=""
for node in $(scontrol show hostnames "$SLURM_NODELIST"); do
  GPU_AFFINITY+="$(srun --ntasks=1 -N 1-1 --gpus-per-task={{ .JobDefinition.GPUsPerNode }} --gpu-bind=none --cpu-bind=none -w "$node" sh -c 'nvidia-smi topo -m | grep -E '"'"'^GPU[0-9]+'"'"' | awk '"'"'{print $1}'"'"' | sed '"'"'s/GPU//'"'"' | tr '"'"'\n'"'"' '"'"':'"'"'')"
  {{- if lt (mul .JobDefinition.GPUsPerNode .JobDefinition.MaxNodes ) 4 }}
  # Add affinity for GPU sharing
  GPU_AFFINITY+="$(srun --ntasks=1 -N 1-1 --gpus-per-task={{ .JobDefinition.GPUsPerNode }} --gpu-bind=none --cpu-bind=none -w "$node" sh -c 'nvidia-smi topo -m | grep -E '"'"'^GPU[0-9]+'"'"' | awk '"'"'{print $1}'"'"' | sed '"'"'s/GPU//'"'"' | tr '"'"'\n'"'"' '"'"':'"'"'')"
  {{- end }}
done
GPU_AFFINITY="${GPU_AFFINITY%:}"
export GPU_AFFINITY

CPU_AFFINITY=""
for node in $(scontrol show hostnames "$SLURM_NODELIST"); do
  CPU_AFFINITY+="$(srun --ntasks=1 -N 1-1 --gpus-per-task={{ .JobDefinition.GPUsPerNode }} --gpu-bind=none --cpu-bind=none -w "$node" sh -c 'nvidia-smi topo -m | grep -E '"'"'^GPU[0-9]+'"'"' | awk '"'"'{print $7}'"'"' | tr '"'"'\n'"'"' '"'"':'"'"'')"
  {{- if lt (mul .JobDefinition.GPUsPerNode .JobDefinition.MaxNodes ) 4 }}
  # Add affinity for GPU sharing
  CPU_AFFINITY+="$(srun --ntasks=1 -N 1-1 --gpus-per-task={{ .JobDefinition.GPUsPerNode }} --gpu-bind=none --cpu-bind=none -w "$node" sh -c 'nvidia-smi topo -m | grep -E '"'"'^GPU[0-9]+'"'"' | awk '"'"'{print $1}'"'"' | sed '"'"'s/GPU//'"'"' | tr '"'"'\n'"'"' '"'"':'"'"'')"
  {{- end }}
done
CPU_AFFINITY="${CPU_AFFINITY%:}"
export CPU_AFFINITY

srun --mpi=pmix_v4 \
  --cpu-bind=none \
  --gpu-bind=none \
  --nodes={{ .JobDefinition.MaxNodes }}-{{ .JobDefinition.MaxNodes }} \
  --ntasks={{ .JobDefinition.NTasks }} \
  --ntasks-per-node={{ .JobDefinition.NTasksPerNode }} \
{{- if ge (mul .JobDefinition.GPUsPerNode .JobDefinition.MaxNodes ) 4 }}
  --gpus-per-task=1 \
{{- end }}
  --container-image="{{ .Image }}" \
  sh -c 'cat << '"'"'EOF'"'"' > /tmp/test.dat \
  && sed -Ei "s/:1//g" ./hpl.sh \
  && ./hpl.sh --xhpl-ai --cpu-affinity $CPU_AFFINITY --cpu-cores-per-rank {{ .BenchmarkParams.CPUsPerTask }} --gpu-affinity $GPU_AFFINITY --dat "/tmp/test.dat"
{{- template "datfile" .BenchmarkParams -}}
EOF'

LOG_FILE="$(scontrol show job $SLURM_JOB_ID | grep "StdOut=" | sed 's/.*StdOut=//g')"

curl -sS \
  --upload-file \
  "$LOG_FILE" \
  -H "X-Secret: {{ .Secret }}" \
  "https://{{ .SupervisorPublicAddress }}/benchmark/{{ .Phase }}"
{{ end -}}
