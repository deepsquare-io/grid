---
title: 'Part 5: Real use case: Machine Learning'
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# Real use case: Machine Learning

Welcome to the fifth part of our tutorial series. After getting comfortable with running basic HPC tasks on the DeepSquare platform, we're now ready to delve into more advanced features. In this tutorial, we'll be harnessing the full potential of our supercomputers through **multi-node distributed training**, powered by [MPI](https://fr.wikipedia.org/wiki/Message_Passing_Interface) and [Horovod](https://github.com/horovod/horovod).

## MPI and DeepSquare

DeepSquare leverages the power of supercomputers, primarily designed for robust high-performance computing tasks, to handle complex and data-intensive deep learning models. These supercomputers, different from standard cloud servers, utilize specialized hardware like GPUs or TPUs and parallel processing techniques for optimal performance.

A critical enabler of this high-performance parallel computing is the Message Passing Interface (MPI). This communication protocol, extensively used in supercomputers, facilitates data exchange and coordination between different processors and nodes during parallel computing tasks. The unique advantages of MPI include scalability, flexibility, low-latency, and high-bandwidth communication, making it an integral component of DeepSquare's ecosystem and many scientific and engineering applications.

## Adapting the ML example for multi-node distributed training

To distribute the training of our machine learning models across multiple nodes, we'll be leveraging [Horovod](https://horovod.ai/), an open-source tool designed for distributed training of deep learning models. Here's how we can adapt our code:

1. **Initialize Horovod:** Begin by initializing Horovod at the start of your script with:

   ```python
   hvd.init()
   ```

   at the initialization of the script.

2. **Assign a GPU per process:** Check if a GPU is available and assign one to each process using `hvd.local_rank()`

   ```python
   if torch.cuda.is_available():
       torch.cuda.set_device(hvd.local_rank())
   ```

3. **Wrap the optimizer:** Use the `hvd.DistributedOptimizer` to wrap your optimizer.

4. **Broadcast the initial variable states:** Ensure all processes are on the same page by broadcasting the initial variable states from rank 0 to all other processes.

   ```python
   hvd.broadcast_parameters(model.state_dict(), root_rank=0)
   hvd.broadcast_optimizer_state(optimizer, root_rank=0)
   ```

5. **Save checkpoints on worker 0 only:** Modify your code to save checkpoints only on **worker 0** to prevent potential corruption from other workers.

As we allocate a GPU per process, the resource **allocation** and **usage** will change accordingly.

Now, let's look at how to incorporate these changes in different types of workflows:

<Tabs groupId="workflow-type">
  <TabItem value="new" label="Workflow">

```json title="Workflow with 2 tasks and 1 GPU per task"
{
  "resources": {
    "tasks": 4,
    "gpusPerTask": 1,
    "cpusPerTask": 8,
    "memPerCpu": 2048
  },
  "enableLogging": true,
  "env": [
    {
      "key": "OMPI_MCA_pml",
      "value": "ucx"
    },
    {
      "key": "OMPI_MCA_btl",
      "value": "^vader,tcp,openib,uct"
    }
  ],
  "input": {
    "http": {
      "url": "https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz"
    }
  },
  "output": {
    "s3": {
      "region": "at-vie-1",
      "bucketUrl": "s3://cifar10",
      "path": "/",
      "accessKeyId": "EXO***",
      "secretAccessKey": "***",
      "endpointUrl": "https://sos-at-vie-1.exo.io"
    }
  },
  "continuousOutputSync": true,
  "steps": [
    {
      "name": "train",
      "run": {
        "command": "/.venv/bin/python3 main.py --horovod --checkpoint_out=$DEEPSQUARE_OUTPUT/ckpt.pth --dataset=$DEEPSQUARE_INPUT/",
        "resources": {
          "tasks": 4
        },
        "container": {
          "image": "deepsquare-io/cifar-10-example:latest",
          "registry": "ghcr.io"
        },
        "workDir": "/app"
      }
    }
  ]
}
```

  </TabItem>
  <TabItem value="resume" label="Workflow (from checkpoint)">

If you're resuming from a checkpoint, here's the workflow configuration:

```json title="Workflow with 2 tasks and 1 GPU per task (from checkpoint)"
{
  "resources": {
    "tasks": 4,
    "gpusPerTask": 1,
    "cpusPerTask": 8,
    "memPerCpu": 2048
  },
  "enableLogging": true,
  "env": [
    {
      "key": "OMPI_MCA_pml",
      "value": "ucx"
    },
    {
      "key": "OMPI_MCA_btl",
      "value": "^vader,tcp,openib,uct"
    }
  ],
  "input": {
    "s3": {
      "region": "at-vie-1",
      "bucketUrl": "s3://cifar10",
      "path": "/",
      "accessKeyId": "EXO***",
      "secretAccessKey": "***",
      "endpointUrl": "https://sos-at-vie-1.exo.io"
    }
  },
  "output": {
    "s3": {
      "region": "at-vie-1",
      "bucketUrl": "s3://cifar10",
      "path": "/",
      "accessKeyId": "EXO***",
      "secretAccessKey": "***",
      "endpointUrl": "https://sos-at-vie-1.exo.io"
    }
  },
  "continuousOutputSync": true,
  "steps": [
    {
      "name": "download dataset",
      "run": {
        "command": "curl -fsSL https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz -o $STORAGE_PATH/cifar-10-python.tar.gz; tar -C $STORAGE_PATH -xvzf $STORAGE_PATH/cifar-10-python.tar.gz; ls -lah $STORAGE_PATH",
        "container": {
          "image": "curlimages/curl:latest",
          "registry": "registry-1.docker.io"
        }
      }
    },
    {
      "name": "train",
      "run": {
        "command": "/.venv/bin/python3 main.py --horovod --checkpoint_in=$DEEPSQUARE_INPUT/ckpt.pth --checkpoint_out=$DEEPSQUARE_OUTPUT/ckpt.pth --dataset=$STORAGE_PATH/",
        "resources": {
          "tasks": 4
        },
        "container": {
          "image": "deepsquare-io/cifar-10-example:latest",
          "registry": "ghcr.io"
        },
        "workDir": "/app"
      }
    }
  ]
}
```

  </TabItem>
</Tabs>

Congrats! You've now learned how to supercharge your HPC workloads with MPI and Horovod on DeepSquare.

The performance of HPC clusters heavily depends on the effectiveness of the message passing interface (MPI) utilized to distribute tasks across the nodes. DeepSquare offers a decentralized network of HPC clusters, eliminating the need to maintain personal infrastructure, while providing an efficient MPI-based communication framework to enable seamless distributed computing.

The combination of DeepSquare's capacity and MPI creates a potent solution for scaling HPC workloads. This approach offers researchers and engineers a cost-effective solution for HPC, enabling them to focus on developing new technologies and unlocking new possibilities without the added burden of infrastructure maintenance.

## What's next?

Although we've finished our getting started guide, there are still some great features you can use to speed up your research and development.

If you want to play around with DeepSquare, you can use [the development environment](https://app.deepsquare.run/sandbox) to dispatch workflows.

You can also read [the guides](/workflow/guides/overview) about the advanced features of DeepSquare.

Want to create your own application with DeepSquare? You might be interested in the [DeepSquare SDK](/workflow/client-development/overview).

Don't forget! If you are lost when writing a workflow, you can use the Workflow API reference as your companion.
